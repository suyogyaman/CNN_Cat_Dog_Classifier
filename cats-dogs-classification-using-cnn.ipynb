{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Idea is to apply CNN to Cats&Dogs Image Classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from zipfile import ZipFile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#test_dir=\"../input/dogs-cats-images/dog vs cat/dataset/test_set\"\n",
    "#train_dir=\"../input/dogs-cats-images/dog vs cat/dataset/training_set\"\n",
    "\n",
    "test_dir=\"dataset/test_set\"\n",
    "train_dir=\"dataset/training_set\"\n",
    "\n",
    "train_dir_cats = train_dir + '/cats'\n",
    "train_dir_dogs = train_dir + '/dogs'\n",
    "test_dir_cats = test_dir + '/cats'\n",
    "test_dir_dogs = test_dir + '/dogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cats training images -  4000\n",
      "number of dogs training images -  4000\n",
      "number of cats testing images -  1000\n",
      "number of dogs testing images -  1000\n"
     ]
    }
   ],
   "source": [
    "print('number of cats training images - ',len(os.listdir(train_dir_cats)))\n",
    "print('number of dogs training images - ',len(os.listdir(train_dir_dogs)))\n",
    "print('number of cats testing images - ',len(os.listdir(test_dir_cats)))\n",
    "print('number of dogs testing images - ',len(os.listdir(test_dir_dogs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the RGB images into array of numbers. The requirement can be satisfied by ImageDataGenerator() https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(rescale = 1.0/255.0, zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "training_data = data_generator.flow_from_directory(directory = train_dir,\n",
    "                                                   target_size = (64, 64),\n",
    "                                                   batch_size = batch_size,\n",
    "                                                   class_mode = 'binary')\n",
    "testing_data = data_generator.flow_from_directory(directory = test_dir,\n",
    "                                                  target_size = (64, 64),\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the layers in the Convolutional Deep Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = training_data.image_shape))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(rate = 0.3))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "model.add(Conv2D(filters = 126, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(rate = 0.15))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units = 32, activation = 'relu'))\n",
    "model.add(Dropout(rate = 0.15))\n",
    "\n",
    "model.add(Dense(units = 64, activation = 'relu'))\n",
    "model.add(Dropout(rate = 0.1))\n",
    "\n",
    "model.add(Dense(units = len(set(training_data.classes)), activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 126)       72702     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 126)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 126)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                145184    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 239,520\n",
      "Trainable params: 239,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-5ddb9c1347a2>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 5 steps, validate for 5 steps\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 15s 3s/step - loss: 0.7010 - accuracy: 0.5000 - val_loss: 0.6948 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.7120 - accuracy: 0.5063 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6998 - accuracy: 0.4625 - val_loss: 0.6932 - val_accuracy: 0.5125\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6878 - accuracy: 0.5312 - val_loss: 0.6930 - val_accuracy: 0.4875\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6967 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "fitted_model = model.fit_generator(training_data,\n",
    "                        steps_per_epoch = 5,\n",
    "                        epochs = 5,\n",
    "                        validation_data = testing_data,\n",
    "                        validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d76cce7988>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracy and validation accuracy\n",
    "accuracy = fitted_model.history['accuracy']\n",
    "plt.plot(range(len(accuracy)), accuracy, 'bo', label = 'accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d76ae21308>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "def testing_image(image_directory):\n",
    "    test_image = image.load_img(image_directory, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = model.predict(x = test_image)\n",
    "    print(result)\n",
    "    print(result[0][0])\n",
    "    print(result[0][1])\n",
    "    if result[0][0]  == 1:\n",
    "        prediction = 'Dog'\n",
    "    else:\n",
    "        prediction = 'Cat'\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03069333 0.9693067 ]]\n",
      "0.030693332\n",
      "0.9693067\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "print(testing_image(test_dir + '/cats/cat.4003.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-460da444203f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00301377 0.9969862 ]]\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "print(testing_image(test_dir + '/dogs/dog.4003.jpg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
